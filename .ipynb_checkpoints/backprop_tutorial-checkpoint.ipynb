{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Backpropogation\n",
    "The general idea of backpropogation is that we cannot determine what a hidden unit should do but we can calculate how the error changes as we change hidden activity. So we are using error derivatives with respect to hidden activities. But remember that each hidden activity has many incoming and outgoing connections which we must take into account when doing the math.\n",
    "\n",
    "First let us define the error as such\n",
    "$$ E = \\frac{1}{2} \\sum_{j \\in output}(t_j-y_j)^2$$\n",
    "\n",
    "Where $t_j$ is our target class and $y_j$ is our model output. This is known as *mean squared error\n",
    "\n",
    "Then we know\n",
    "$$\\frac{\\partial E}{\\partial y_j} = -(t_j-y_j)$$\n",
    "\n",
    "This gives us an error derivative for when we change the activity of an output unit $y_j$. So now that we have computed these error derivatives for the output layer we want to somehow use this to solve for the error derivatives in the layer before it. \n",
    "\n",
    "For notations sake assume layer $i$ comes BEFORE layer $j$ in the feed forward process, let $y_i$ be the output of some hidden unit $i$ in layer $i$ and $y_j$ be the output of node $j$ in layer $j$. Then define the sum of all the activities inputs into unit $j$ as $z_j$. Then we can calculate\n",
    "$$\\frac{\\partial E}{\\partial z_j} = \\frac{dy_j}{dz_j}\\frac{\\partial E}{\\partial y_j} = y_j(1-y_j)\\frac{\\partial E}{\\partial y_j}$$\n",
    "\n",
    "The above is just using the chain rule of calculus. Now we can compute the error derivative with respect to the output of unit $i$ and not the entire sum of connections.\n",
    "$$\\frac{\\partial E}{\\partial y_i} = \\sum_{j}\\frac{dz_j}{dy_i}\\frac{\\partial E}{\\partial z_j} = \\sum_{j}w_{ij}\\frac{\\partial E}{\\partial z_j}$$\n",
    "\n",
    "The above is deriving the first part of the equation I showed you guys in class. We simply multiply all the errors in the layer ahead of us by the connections we have to those units because that is how much we \"contributed\" to that error. But that is not all we want, we want to find out how much error our weights contributed as well. We can calculate that easily too now!\n",
    "$$\\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial z_j}{\\partial w_{ij}}\\frac{\\partial E}{\\partial z_j} = y_j\\frac{\\partial E}{\\partial z_j}$$\n",
    "\n",
    "Now to show a concrete example, take the derivative of our sigmoid(or logistic) function and you should get \n",
    "$$\\frac{\\partial y}{\\partial z}(\\frac{1}{1+e^{-z}}) = y(1-y)$$\n",
    "\n",
    "If you still have questions/ concerns or want to learn more I HIGHLY reccomend the classes listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[Andrew Ng Coursera Machine Learning Course](https://www.youtube.com/watch?v=mcnvIWDnPns&index=44&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW) - The most relavent is the machine learning W4 lectures. This will go over neural networks in general but not much of the math. Also more emphasis on matrix computation than math.\n",
    "\n",
    "[Geoffrey Hinton Coursera Machine Learning Course](https://class.coursera.org/neuralnets-2012-001/lecture) - Lecture 3 is the most relavent for backpropogation but I HIGHLY HIGHLY HIGHLY HIGHLY reccomend watching more of them.....HIGHLY. These were made by the founder and god of neural networks so you can't find a better source than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
